# Project Structure

Complete overview of the V1 Vision Pipeline file organization.

## File Tree

```
blindsight/
â”‚
â”œâ”€â”€ init-cv/                                  # Original video receiving scripts
â”‚   â”œâ”€â”€ receive.py                            # Basic Pi video receiver
â”‚   â””â”€â”€ cv_receive_edges_luminance.py         # Example with CV processing
â”‚
â”œâ”€â”€ MDPI2021/                                 # V1 cortex model (external)
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ LIFL_IE/                              # NEST custom neuron module
â”‚   â”‚   â”œâ”€â”€ lifl_psc_exp_ie.cpp               # Neuron model implementation
â”‚   â”‚   â”œâ”€â”€ lifl_psc_exp_ie.h                 # Header
â”‚   â”‚   â”œâ”€â”€ CMakeLists.txt                    # Build configuration
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ Examples/
â”‚       â””â”€â”€ V1 Oriented Columns comapred with MEG/
â”‚           â”œâ”€â”€ OrientedColumnV1.py           # V1 column definition
â”‚           â”œâ”€â”€ Simulation_V1_pinwheel_MEGcomparison.py
â”‚           â””â”€â”€ files/                        # Pre-trained weights
â”‚               â”œâ”€â”€ soma_exc_0.pckl
â”‚               â”œâ”€â”€ soma_exc_45.pckl
â”‚               â”œâ”€â”€ soma_exc_90.pckl
â”‚               â””â”€â”€ soma_exc_135.pckl
â”‚
â””â”€â”€ v1_vision/                                # â† OUR PIPELINE (THIS DIRECTORY)
    â”‚
    â”œâ”€â”€ README.md                             # Full documentation
    â”œâ”€â”€ QUICKSTART.md                         # 5-minute setup guide
    â”œâ”€â”€ ARCHITECTURE.md                       # System design details
    â”œâ”€â”€ PROJECT_STRUCTURE.md                  # This file
    â”œâ”€â”€ requirements.txt                      # Python dependencies
    â”‚
    â”œâ”€â”€ __init__.py                           # Package initialization
    â”œâ”€â”€ config.py                             # âš™ï¸ All configuration parameters
    â”‚
    â”œâ”€â”€ gabor_feature_extractor.py            # ğŸ” Extract oriented edges
    â”œâ”€â”€ spike_encoder.py                      # âš¡ Convert to spike trains
    â”œâ”€â”€ v1_model_interface.py                 # ğŸ§  V1 cortex simulation
    â”œâ”€â”€ v1_decoder.py                         # ğŸ¨ Reconstruct visual output
    â”œâ”€â”€ visualization.py                      # ğŸ“Š Display utilities
    â”‚
    â”œâ”€â”€ realtime_pipeline.py                  # â–¶ï¸ MAIN SCRIPT - Run this!
    â””â”€â”€ test_static_image.py                  # ğŸ§ª Test with static images
```

## Core Files (Required)

### 1. **config.py** âš™ï¸
**Purpose**: Central configuration for entire pipeline
**Contains**:
- Video stream settings (IP, port, resolution)
- Spatial grid parameters (18Ã—18 = 324 neurons)
- Gabor filter settings (orientations, wavelength, sigma)
- Spike encoding parameters (rate/latency/hybrid)
- V1 model settings (simulation time, layers)
- Visualization options (which windows to show)
- Performance tuning (downsampling, frame skipping)

**Edit this first** to configure for your setup!

### 2. **gabor_feature_extractor.py** ğŸ”
**Purpose**: Extract orientation-selective visual features
**Key Class**: `GaborFeatureExtractor`
**Methods**:
- `extract_features(frame)` â†’ Returns responses for 324 neurons Ã— 4 orientations
- `visualize_gabor_responses()` â†’ Creates 2Ã—2 grid of filtered images
- `visualize_receptive_fields()` â†’ Draws grid on frame

**What it does**: Applies Gabor filters (edge detectors) at 4 orientations to 324 spatial locations.

### 3. **spike_encoder.py** âš¡
**Purpose**: Convert visual features to neural spike trains
**Key Class**: `SpikeEncoder`
**Methods**:
- `encode(features)` â†’ Generates spike trains
- `format_for_nest()` â†’ Formats for NEST simulator

**Encoding Types**:
- **Rate**: More spikes for stronger features
- **Latency**: Earlier spikes for stronger features
- **Hybrid**: Both combined

### 4. **v1_model_interface.py** ğŸ§ 
**Purpose**: Interface to NEST-based V1 cortex model
**Key Class**: `V1ModelInterface`
**Methods**:
- `setup_model()` â†’ Creates 4 orientation columns
- `inject_spikes(spike_trains)` â†’ Injects input
- `run_simulation()` â†’ Runs NEST simulation
- `get_output()` â†’ Returns V1 spikes
- `calculate_orientation_selectivity()` â†’ Analyzes responses

**What it does**: Simulates ~5,000 cortical neurons processing visual input.

### 5. **v1_decoder.py** ğŸ¨
**Purpose**: Reconstruct visual representations from V1 spikes
**Key Class**: `V1Decoder`
**Methods**:
- `decode(v1_output)` â†’ Creates visualizations
- `create_orientation_map()` â†’ Color-coded orientation preferences
- `create_activity_map()` â†’ Neural activity heatmap
- `reconstruct_edges()` â†’ Oriented line segments

**What it does**: Interprets V1 output and creates human-readable visualizations.

### 6. **visualization.py** ğŸ“Š
**Purpose**: Display utilities and performance monitoring
**Key Classes**:
- `SpikeRasterPlot` â†’ Real-time spike visualization
- `PipelineMonitor` â†’ FPS and timing statistics
- `MultiWindowDisplay` â†’ Manages multiple windows

### 7. **realtime_pipeline.py** â–¶ï¸
**Purpose**: MAIN SCRIPT - Orchestrates entire pipeline
**Key Class**: `V1VisionPipeline`
**What it does**:
1. Receives video from Pi
2. Extracts Gabor features
3. Encodes to spikes
4. Runs V1 simulation
5. Decodes output
6. Displays all stages

**Run this to start the pipeline!**

### 8. **test_static_image.py** ğŸ§ª
**Purpose**: Test pipeline with static images (no Pi needed)
**What it does**:
- Tests each component independently
- Useful for debugging
- Can use generated test images or your own

**Run this first** to verify installation!

## Documentation Files

### **README.md**
Complete documentation:
- Architecture overview
- Setup instructions
- Configuration guide
- Troubleshooting
- Technical details

### **QUICKSTART.md**
Get running in 5 minutes:
- Installation steps
- Quick test
- Common issues

### **ARCHITECTURE.md**
System design:
- Data flow diagrams
- Component interactions
- Performance analysis
- Extension points

### **PROJECT_STRUCTURE.md** (this file)
File organization and purpose of each component.

## Configuration Files

### **requirements.txt**
Python package dependencies:
```
numpy
opencv-python
nest-simulator
matplotlib (optional)
```

### **__init__.py**
Makes v1_vision a Python package. Exports main classes.

## Usage Patterns

### Quick Test (No Pi)
```bash
python test_static_image.py
```

### Full Pipeline
```bash
# 1. Edit config.py (set Pi IP)
# 2. Start Pi camera stream
# 3. Run pipeline
python realtime_pipeline.py
```

### Import as Package
```python
from v1_vision import (
    GaborFeatureExtractor,
    SpikeEncoder,
    V1ModelInterface,
    V1Decoder
)

# Use components individually
extractor = GaborFeatureExtractor()
features = extractor.extract_features(my_image)
```

## External Dependencies

### MDPI2021 V1 Model
- Located in `../MDPI2021/`
- Contains pre-trained cortical column
- Requires NEST simulator
- Custom LIFL_IE neuron module

**Must be compiled separately!** See `MDPI2021/LIFL_IE/` for instructions.

### Pre-trained Weights
Located in: `MDPI2021/Examples/V1 Oriented Columns comapred with MEG/files/`
- `soma_exc_0.pckl` â†’ 0Â° orientation column weights
- `soma_exc_45.pckl` â†’ 45Â° orientation column weights  
- `soma_exc_90.pckl` â†’ 90Â° orientation column weights
- `soma_exc_135.pckl` â†’ 135Â° orientation column weights

These files contain trained intrinsic excitability parameters.

## Typical Workflow

### Development
1. Edit `config.py` â†’ adjust parameters
2. Run `test_static_image.py` â†’ verify changes
3. Run `realtime_pipeline.py` â†’ test on video
4. Iterate

### Deployment
1. Configure Pi IP in `config.py`
2. Start Pi camera stream
3. Run `realtime_pipeline.py`
4. Monitor performance (FPS display)

### Debugging
1. Use `test_static_image.py` with known images
2. Check each stage output window
3. Review spike counts and firing rates
4. Adjust encoding parameters in `config.py`

## File Relationships

```
config.py (parameters)
    â†“
gabor_feature_extractor.py (uses config)
    â†“
spike_encoder.py (uses config)
    â†“
v1_model_interface.py (uses config, imports OrientedColumnV1.py)
    â†“
v1_decoder.py (uses config)
    â†“
visualization.py (uses config)
    â†“
realtime_pipeline.py (orchestrates all, uses config)
```

All components read from `config.py`, allowing centralized parameter tuning.

## Size Reference

| File | Lines | Purpose |
|------|-------|---------|
| config.py | 94 | Configuration |
| gabor_feature_extractor.py | 247 | Feature extraction |
| spike_encoder.py | 244 | Spike encoding |
| v1_model_interface.py | 247 | V1 simulation |
| v1_decoder.py | 235 | Output decoding |
| visualization.py | 290 | Display utilities |
| realtime_pipeline.py | 313 | Main integration |
| test_static_image.py | 207 | Testing |
| **Total** | **~1,877 lines** | Complete pipeline |

## Getting Help

1. **Setup issues** â†’ See QUICKSTART.md
2. **Understanding system** â†’ See ARCHITECTURE.md
3. **Configuration** â†’ See README.md
4. **Code questions** â†’ Read inline comments (all files well-documented)
5. **Performance** â†’ See ARCHITECTURE.md performance section

## Next Steps

1. âœ… Read QUICKSTART.md
2. âœ… Install dependencies (`pip install -r requirements.txt`)
3. âœ… Test with static image (`python test_static_image.py`)
4. âœ… Configure Pi IP in `config.py`
5. âœ… Run full pipeline (`python realtime_pipeline.py`)
6. âœ… Experiment with parameters in `config.py`
7. âœ… Read ARCHITECTURE.md for deeper understanding

Enjoy exploring biologically-inspired vision! ğŸ§ ğŸ‘ï¸

